{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## project starts from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# make sure plots are displayed correctly\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating weights\n",
    "From the lecture slide, we know that input-to-reservoir weight $Win$ and reservoir weight $Wr$ are drawn independently from a uniform distribution in $[-1,1]$.\n",
    "As $Wr$ directly impacts on the dynamics of the network. I follow the first rule of the lecture for generating $Wr$.\n",
    "\n",
    "$Wr = a \\frac{Wr}{\\rho{(Wr)}}$\n",
    "\n",
    "Here, $\\rho{(Wr)}$ is the Spectral radius which is the largest absolute value of its eigenvalues. $a$ is a scaler hyper-parameter. Here, a is evaluated in range 0.1 to 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_weights(a, inshape, reservior):\n",
    "    np.random.seed(62)\n",
    "    Win = (np.random.rand(reservior, inshape) - 0.5) * 1\n",
    "    Wr = np.random.rand(reservior, reservior) - 0.5\n",
    "    #a = 0.99\n",
    "    print('Computing spectral radius...')\n",
    "    rhoW = max(abs(linalg.eig(Wr)[0]))\n",
    "    print('done.')\n",
    "    print(rhoW)\n",
    "    Wr *= a / rhoW\n",
    "\n",
    "    return Win, Wr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(label,pred):\n",
    "    loss = np.square(np.subtract(label,pred)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESN Network\n",
    "1. creating the input-output pair. let t be the input time series. I have created a output pair y=t+1. Thus, the input-output pair is ${(t,y)}_{i=1}^{N-1}$.\n",
    "\n",
    "2. Split the data into train_pair and test_pair, where ${(train\\_pair)}_{1}^{N-1-K}$ and ${(test\\_pair)}_{N-1-K}^{N-1}$.\n",
    "\n",
    "3. Genarate the input-to-reservoir weight  $Win$  and reservoir weight $Wr$ using the method discussed above.\n",
    "\n",
    "4. Create matrix $X \\in R^{N \\times N_r}$, where, $N=N-1$ and $N_r = $size of the reservior.\n",
    "\n",
    "5. Intialize the first state x=0\n",
    "\n",
    "6. Create input matrix $out = R^{len(train\\_pair)}$\n",
    "\n",
    "6. For each t, update the state $x_i$ as follows:\n",
    "$x_t = tanh(Wr x_{t-1} + Win t)$\n",
    "then collect all the resulting states in $X$.\n",
    "\n",
    "7. Calculate the Wout using least-square problem as follows:\n",
    "$Wout = (X^T X)^{-1}  X^T out$ as the inverse operation takes alot of computation resource. I have used scipy.linalg.solve(). Thus, the equation becomes\n",
    "$ (X X^T) Wout = (X out^{T})^T$\n",
    "\n",
    "these steps are upto the training phase and finding out Wout.\n",
    "\n",
    "Here begins the k-step prediction.\n",
    "\n",
    "8. Create prediction matrix $Y \\in R^K$.\n",
    "\n",
    "9. Initialize first time step $t=train\\_pair[-1,1]$ i.e. last time-step data of the training set. The subsequent data will be predicted by the model.\n",
    "\n",
    "10. For each t, update the state $x_i$ as follows:\n",
    "$x_t = tanh(Wr x_{t-1} + Win t)$ then\n",
    "$ y = Wout x_t$.  Finally collect all the prediction in $Y$.\n",
    "\n",
    "11. Calculate the mean square loss between the ground truth time step and predicted time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ESN(name, reservior = 300, k = 20, a=0.99):\n",
    "    inx = 1\n",
    "    outx = 1\n",
    "    \n",
    "    # creating the input output data\n",
    "    data = np.loadtxt(str(name)+'.txt')\n",
    "\n",
    "    iopair = np.zeros((len(data) - 1, 2), dtype=float)\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        iopair[i, 0] = data[i]\n",
    "        iopair[i, 1] = data[i + 1]\n",
    "\n",
    "    # k step prediction\n",
    "    split = iopair.shape[0] - k\n",
    "    train_pair = iopair[:split, :]\n",
    "    test_pair = iopair[split:, :]\n",
    "    plot_label = data[split:]\n",
    "\n",
    "    # generate the random weights\n",
    "    Win, Wr = generate_weights(a, inx, reservior)\n",
    "\n",
    "\n",
    "    # resulting N states\n",
    "    X = np.zeros((train_pair.shape[0], reservior))\n",
    "\n",
    "    # set the target matrix\n",
    "    out = np.array(train_pair[:, 1])\n",
    "    out = np.expand_dims(out, axis=0)\n",
    "\n",
    "    # initialize the first state\n",
    "    x = np.zeros((reservior, 1))\n",
    "\n",
    "    # ESN layer\n",
    "    for t in range(train_pair.shape[0]):\n",
    "        u = np.array(train_pair[t, 0])\n",
    "        x = np.tanh(np.dot(Win, u) + np.dot(Wr, x))\n",
    "        X[t, :] = x.squeeze()\n",
    "\n",
    "    # calculating Wout\n",
    "    X = X.T\n",
    "    Wout = linalg.solve(np.dot(X, X.T), np.dot(X, out.T)).T\n",
    "\n",
    "    Y = np.zeros((outx,k))\n",
    "    u = np.array(train_pair[-1,1])\n",
    "    for t in range(test_pair.shape[0]):\n",
    "        x = np.tanh( np.dot( Win,u) + np.dot( Wr, x ))\n",
    "        y = np.dot( Wout, x )\n",
    "        Y[:,t] = y\n",
    "        u = y\n",
    "\n",
    "    label =test_pair[:,1]\n",
    "    label = np.expand_dims(label, axis=0)\n",
    "\n",
    "    loss = MSE(label,Y)\n",
    "    \n",
    "    return loss, plot_label, Y, X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All plots function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_3d(name, out, k, reservior, a):\n",
    "    \n",
    "    x = out[:,0]\n",
    "    y = out[:,1]\n",
    "    z = out[:,2]\n",
    "    \n",
    "    # Creating figure\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    ax.scatter3D(x, y, z, cmap='Greens')\n",
    "\n",
    "    # Creating plot\n",
    "    #ax.scatter3D(x, y, z, color=\"green\")\n",
    "    plt.title(name + \" for k= \" + str(k) + \" state = \" +str(reservior) + \" a= \" + str(a))\n",
    "\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction(name, label, prediction,k, reservior, a):\n",
    "    plt.figure(10).clear()\n",
    "    plt.plot( label.T, 'g' )\n",
    "    plt.plot( prediction.T, 'b' )\n",
    "    plt.title(name + ' prediction for k = ' + str(k) + \" state = \" +str(reservior) + \" a = \" + str(a))\n",
    "    plt.legend(['label signal', 'predicted signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA(X, num_components):\n",
    "    # Remove mean from the data\n",
    "    X_remove = X - np.mean(X, axis=0)\n",
    "\n",
    "    # compute sample convariance matrix\n",
    "    cov_mat = np.cov(X_remove, rowvar=False)\n",
    "\n",
    "    # compute eigen decomposition\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "    # sort the eigen values in decreaing order\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:, sorted_index]\n",
    "\n",
    "    # select the number of components to be preserved\n",
    "    eigenvector_subset = sorted_eigenvectors[:, 0:num_components]\n",
    "\n",
    "    # project into selected components\n",
    "    out = np.dot(eigenvector_subset.transpose(), X_remove.transpose()).transpose()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot of data for lorentz\n",
    "data = np.loadtxt('lorentz.txt')\n",
    "# plot some of it\n",
    "plt.figure(10).clear()\n",
    "plt.plot(data[15000:])\n",
    "plt.title('A sample of data of lorentz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot of data for lorentz\n",
    "data = np.loadtxt('2sin.txt')\n",
    "# plot some of it\n",
    "plt.figure(10).clear()\n",
    "plt.plot(data[1000:2000])\n",
    "plt.title('A sample of data of 2sin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Lorentz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'lorentz'\n",
    "k_all = [5, 15, 25, 35, 40]\n",
    "reservior_all = [100, 200, 300, 400, 500, 600, 700, 800 ]\n",
    "a_all = [0.1, 0.2, 0.3, 0.4, 0.5, 0.60, 0.7, 0.8, 0.9, 1, 1.10, 1.2, 1.3]\n",
    "\n",
    "for k in k_all:\n",
    "    for reservior in reservior_all:\n",
    "        for a in a_all:\n",
    "            loss, label, prediction, X = ESN(name, reservior, k, a)\n",
    "            print(\"loss at K = \" + str(k) + \" state = \" +str(reservior) + \" a = \" + str(a) + \" -->\" + str(loss))\n",
    "            plot_prediction(name, label, prediction, k, reservior, a)\n",
    "            out = PCA(X,3)\n",
    "            plot_3d('PCA', out, k, reservior, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis of Lorentz\n",
    "\n",
    "1. The first obersvation is that we get a good reconstruction of lorentz attractor around the value of $a \\simeq  1$ which is expected according to the theoritical knowledge of the lecture.\n",
    "\n",
    "2. By changing the hyper-parameters, the reconstruction changes. For fixed k and a, the changes of reservior size changes the reconstruction. This is applicable for fixed reservior and a and fixed reservior and k.\n",
    "\n",
    "3. It is also evident that the reconstruction is stable under small perturbations of the hyper-parameters and random initialization of the developed ESN model. For k =  5 and reservior = 700, the reconstruction is stable for $a = 1$ to $a=1.1$.\n",
    "\n",
    "4. Given the proper hyper-parameter settings, the ESN model can perfectly predict the choatic time series as it able to predict $k = 40$ time steps for $ reservior = 700 $ and $ a =1 $ as well as can learn the embedding provided by the lorentz attractor visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on 2sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = '2sin'\n",
    "k_all = [5, 15, 25, 35, 40]\n",
    "reservior_all = [100, 200, 300, 400, 500 ]\n",
    "a_all = [ 0.5, 0.60, 0.7, 0.8, 0.9, 1, 1.10, 1.2, 1.3]\n",
    "\n",
    "for k in k_all:\n",
    "    for reservior in reservior_all:\n",
    "        for a in a_all:\n",
    "            loss, label, prediction, X = ESN(name, reservior, k, a)\n",
    "            print(\"loss at K = \" + str(k) + \" state = \" +str(reservior) + \" a = \" + str(a) + \" -->\" + str(loss))\n",
    "            plot_prediction(name, label, prediction, k, reservior, a)\n",
    "            out = PCA(X,3)\n",
    "            plot_3d('PCA', out, k, reservior, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis of 2sin\n",
    "The analysis is same as the lorentz. Although the instruction doesn't tell to show the principle components of the 2sin. I visualize to see the embedding pattern. The model can perpectly predict 40 timesteps of 2sin signal at $reservior = 400$ and $ a = 1.3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Analysis\n",
    "I just compare with the LLE because of low computation. The construction very bad as expected because LLE tries to preserve local information. For compare with other dimentionality reduction methods, run the last block of the code on heavy computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "embedding = LocallyLinearEmbedding(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'lorentz'\n",
    "reservior = 700\n",
    "k = 40\n",
    "a = 1\n",
    "loss, label, prediction, X = ESN(name, reservior, k, a)\n",
    "print(\"loss at K = \" + str(k) + \" state = \" +str(reservior) + \" a = \" + str(a) + \" -->\" + str(loss))\n",
    "plot_prediction(name, label, prediction, k, reservior, a)\n",
    "out = PCA(X,3)\n",
    "plot_3d('PCA', out, k, reservior, a)\n",
    "\n",
    "out_LLE = embedding.fit_transform(X)\n",
    "plot_3d('LLE', out_LLE, k, reservior, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "KPCA = KernelPCA(n_components=3, kernel='linear')\n",
    "out_KPCA = KPCA.fit_transform(X)\n",
    "plot_3d('KPCA', out_KPCA, k, reservior, a)\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "isomap = Isomap(n_components=3)\n",
    "out_isomap = isomap.fit_transform(X)\n",
    "plot_3d('ISOMAP', out_isomap, k, reservior, a)\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=3)\n",
    "out_mds = embedding.fit_transform(X)\n",
    "plot_3d('MDS', out_mds, k, reservior, a)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "out_tsne = TSNE(n_components=3, learning_rate='auto', init='random').fit_transform(X)\n",
    "plot_3d('TSNE', out_tsne, k, reservior, a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
